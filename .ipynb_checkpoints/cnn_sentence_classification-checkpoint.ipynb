{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#파일에서 데이터 로드\n",
    "title_arr = []\n",
    "ctr_arr = []\n",
    "with open('../data/title_ctr.csv','r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break;\n",
    "        title_info = line.split(',')\n",
    "        title = title_info[2]\n",
    "        if len(title) > 0:\n",
    "            title_arr.append(title)\n",
    "            ctr_arr.append(float(title_info[3].replace('\\n','')))\n",
    "        \n",
    "print(title_arr[:10])\n",
    "print(ctr_arr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#명사 추출\n",
    "from konlpy.tag import Twitter\n",
    "twitter_nlp = Twitter()\n",
    "\n",
    "title_noun_arr = []\n",
    "for index, title in enumerate(title_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print('step:',index)\n",
    "    title_noun_arr.append(twitter_nlp.nouns(title)) #명사 추출\n",
    "print(title_noun_arr[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#벡터화 using word2vec\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "MIN_NOUN_VECTOR_VALUE = -10.0\n",
    "MAX_NOUN_VECTOR_VALUE = 10.0\n",
    "NOUN_VECTOR_SIZE = 300\n",
    "\n",
    "def generate_random_noun_vector():\n",
    "    return np.random.uniform(low=MIN_NOUN_VECTOR_VALUE, high=MAX_NOUN_VECTOR_VALUE, size=(NOUN_VECTOR_SIZE,))\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec.load('../data/w2v_model_wiki_kor')\n",
    "w2v_model['남자']\n",
    "\n",
    "title_noun_vector_arr = []\n",
    "for index, title_nouns in enumerate(title_noun_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print('step:',index)\n",
    "    noun_vector_arr = []\n",
    "    for noun in title_nouns:\n",
    "        try:\n",
    "            noun_vector = w2v_model[noun]\n",
    "        except Exception as e:\n",
    "            noun_vector = generate_random_noun_vector()\n",
    "        noun_vector_arr.append(noun_vector)\n",
    "    title_noun_vector_arr.append(noun_vector_arr)\n",
    "\n",
    "# print(title_noun_vector_arr[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pedding\n",
    "\n",
    "TITLE_LENGTH = 25\n",
    "\n",
    "def generate_zero_noun_vector():\n",
    "    return np.random.uniform(low=0.0, high=0.0, size=(NOUN_VECTOR_SIZE,))\n",
    "\n",
    "for index, title_noun_vector in enumerate(title_noun_vector_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print('step:',index)\n",
    "    while len(title_noun_vector) < 25:\n",
    "        title_noun_vector.append(generate_zero_noun_vector())\n",
    "    title_noun_vector = title_noun_vector[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ctr to classification\n",
    "NUM_CLASSES = 3\n",
    "ctr_class_arr = []\n",
    "ctr_class_count = [0,0,0]\n",
    "for index, ctr in enumerate(ctr_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print('step:',index)\n",
    "    if ctr < 0.007:\n",
    "        ctr_class_arr.append([0,0,1])\n",
    "        ctr_class_count[2] += 1\n",
    "    elif ctr < 0.012:\n",
    "        ctr_class_arr.append([0,1,0])\n",
    "        ctr_class_count[1] += 1\n",
    "    else:\n",
    "        ctr_class_arr.append([1,0,0])\n",
    "        ctr_class_count[0] += 1\n",
    "\n",
    "print(ctr_class_count)\n",
    "print(ctr_class_arr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_sizes = [3,4,5]\n",
    "embedding_size = 300\n",
    "num_filters =100\n",
    "l2_reg_lambda = 0.0\n",
    "learning_rate = 1e-3\n",
    "\n",
    "input_x = tf.placeholder(tf.float32, [None, TITLE_LENGTH, embedding_size], name=\"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [None, NUM_CLASSES], name=\"input_y\")\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "input_x_expanded = tf.expand_dims(input_x, -1)\n",
    "l2_loss = tf.constant(0.0)\n",
    "\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "    \n",
    "        filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "        conv = tf.nn.conv2d(\n",
    "                            input_x_expanded,\n",
    "                            W,\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding=\"VALID\",\n",
    "                            name=\"conv\")\n",
    "        h = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"activate\")\n",
    "        pooled = tf.nn.max_pool(\n",
    "                            h,\n",
    "                            ksize=[1, TITLE_LENGTH - filter_size + 1, 1, 1],\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding='VALID',\n",
    "                            name=\"pool\")\n",
    "        pooled_outputs.append(pooled)\n",
    "\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "h_pool = tf.concat(pooled_outputs, 3)  #Tensor(\"concat_1:0\", shape=(?, 1, 1, 300), dtype=float32)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])  #Tensor(\"Reshape:0\", shape=(?, 300), dtype=float32)\n",
    "\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "    \n",
    "# Final (unnormalized) scores and predictions\n",
    "with tf.name_scope(\"output\"):\n",
    "    W = tf.get_variable(\n",
    "        \"W\",\n",
    "        shape=[num_filters_total, NUM_CLASSES],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]), name=\"b\")\n",
    "    l2_loss += tf.nn.l2_loss(W)\n",
    "    l2_loss += tf.nn.l2_loss(b)\n",
    "    scores = tf.nn.xw_plus_b(h_pool_flat, W, b, name=\"scores\")\n",
    "    predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "\n",
    "# # CalculateMean cross-entropy loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "    loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "    l2_loss += tf.nn.l2_loss(W)\n",
    "    l2_loss += tf.nn.l2_loss(b)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "with tf.name_scope(\"optimizing\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "# # Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training data, test data 분리\n",
    "test_data_size = 3000\n",
    "train_data_size = len(title_noun_vector_arr)-test_data_size\n",
    "\n",
    "train_input = title_noun_vector_arr[0:train_data_size]\n",
    "train_label = ctr_class_arr[0:train_data_size]\n",
    "test_input = title_noun_vector_arr[train_data_size:]\n",
    "test_label = ctr_class_arr[train_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "index = 0\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter('tensorboard/train',sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('tensorboard/test')\n",
    "    sess.run(init)\n",
    "    for step in range(100):\n",
    "        for start, end in zip(range(0, len(train_input), batch_size), range(batch_size, len(train_input), batch_size)):\n",
    "            cost, acc, tb_summary, h_pool, _ = sess.run([loss, accuracy, merged, h_pool_flat, optimizer], feed_dict={input_x: train_input[start:end], input_y: train_label[start:end]}, dropout_keep_prob: 0.5)\n",
    "            if index % 100 == 0:\n",
    "                train_writer.add_summary(tb_summary, index)\n",
    "                print('index:',index)\n",
    "                print('cost:',cost)\n",
    "                print('acc:',acc)\n",
    "                print('---------')\n",
    "            index += 1\n",
    "        t_cost, t_acc, t_tb_summary, t_h_pool = sess.run([loss, accuracy, merged, h_pool_flat], feed_dict={input_x: test_input, input_y: test_label}, dropout_keep_prob: 1)\n",
    "        test_writer.add_summary(t_tb_summary, step)\n",
    "        print('============')\n",
    "        print('step:',step)\n",
    "        print('cost:',t_cost)\n",
    "        print('acc:',t_acc)\n",
    "        print('h_pool:',t_h_pool)\n",
    "        print('============')\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: drop-out 적용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
