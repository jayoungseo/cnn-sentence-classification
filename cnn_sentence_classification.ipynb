{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['초강력 워터프루프', '셔츠 초간단 연출법', '센스 넘치는 여행 소품들', '부위별 셀프 운동법', '이국주 퍼스널리티 오전', '강동원 인형놀이 오후', '호위무사 박수경 팬클럽까지 예쁘면 모든게 용서되나 경악 저녁', '빈 라덴 사살됐다 살아있다 여전히 풀리지 않는 미스터리 저녁', '남친에게 이런 짓까지 미녀스타의 충격적 실체 저녁', '택배등기 반송메시지 조심해야 하는 이유 저녁']\n",
      "[0.0211, 0.0144, 0.0091, 0.0209, 0.0177, 0.0091, 0.0338, 0.0142, 0.0901, 0.0148]\n"
     ]
    }
   ],
   "source": [
    "#파일에서 데이터 로드\n",
    "title_arr = []\n",
    "ctr_arr = []\n",
    "with open('../data/title_ctr.csv','r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break;\n",
    "        title_info = line.split(',')\n",
    "        title = title_info[2]\n",
    "        if len(title) > 0:\n",
    "            title_arr.append(title)\n",
    "            ctr_arr.append(float(title_info[3].replace('\\n','')))\n",
    "        \n",
    "print(title_arr[:10])\n",
    "print(ctr_arr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "step: 10000\n",
      "step: 20000\n",
      "step: 30000\n",
      "step: 40000\n",
      "step: 50000\n",
      "[['초', '강력', '워터', '프루프'], ['셔츠', '초', '간단', '연출', '법'], ['센스', '여행', '소품'], ['부위', '별', '셀프', '운동', '법'], ['이국주', '퍼스', '널리', '티', '오전'], ['강동원', '인형', '오후'], ['호위', '무사', '박수경', '팬클럽', '모든', '용서', '경악', '저녁'], ['빈', '라덴', '사살', '미스터리', '저녁'], ['남친', '짓', '미녀', '스타', '충격', '실체', '저녁'], ['택배', '등기', '반송', '메시지', '이유', '저녁']]\n"
     ]
    }
   ],
   "source": [
    "#명사 추출\n",
    "from konlpy.tag import Twitter\n",
    "twitter_nlp = Twitter()\n",
    "\n",
    "title_noun_arr = []\n",
    "for index, title in enumerate(title_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print('step:',index)\n",
    "    title_noun_arr.append(twitter_nlp.nouns(title)) #명사 추출\n",
    "print(title_noun_arr[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "step: 10000\n",
      "step: 20000\n",
      "step: 30000\n",
      "step: 40000\n",
      "step: 50000\n"
     ]
    }
   ],
   "source": [
    "#벡터화 using word2vec\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "MIN_NOUN_VECTOR_VALUE = -10.0\n",
    "MAX_NOUN_VECTOR_VALUE = 10.0\n",
    "NOUN_VECTOR_SIZE = 300\n",
    "\n",
    "def generate_random_noun_vector():\n",
    "    return np.random.uniform(low=MIN_NOUN_VECTOR_VALUE, high=MAX_NOUN_VECTOR_VALUE, size=(NOUN_VECTOR_SIZE,))\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec.load('../data/w2v_model_wiki_kor')\n",
    "w2v_model['남자']\n",
    "\n",
    "title_noun_vector_arr = []\n",
    "for index, title_nouns in enumerate(title_noun_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print('step:',index)\n",
    "    noun_vector_arr = []\n",
    "    for noun in title_nouns:\n",
    "        try:\n",
    "            noun_vector = w2v_model[noun]\n",
    "        except Exception as e:\n",
    "            noun_vector = generate_random_noun_vector()\n",
    "        noun_vector_arr.append(noun_vector)\n",
    "    title_noun_vector_arr.append(noun_vector_arr)\n",
    "\n",
    "# print(title_noun_vector_arr[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "step: 10000\n",
      "step: 20000\n",
      "step: 30000\n",
      "step: 40000\n",
      "step: 50000\n"
     ]
    }
   ],
   "source": [
    "#pedding\n",
    "\n",
    "TITLE_LENGTH = 25\n",
    "\n",
    "def generate_zero_noun_vector():\n",
    "    return np.random.uniform(low=0.0, high=0.0, size=(NOUN_VECTOR_SIZE,))\n",
    "\n",
    "for index, title_noun_vector in enumerate(title_noun_vector_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print('step:',index)\n",
    "    while len(title_noun_vector) < 25:\n",
    "        title_noun_vector.append(generate_zero_noun_vector())\n",
    "    title_noun_vector = title_noun_vector[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "step: 10000\n",
      "step: 20000\n",
      "step: 30000\n",
      "step: 40000\n",
      "step: 50000\n",
      "[17005, 16347, 18116]\n",
      "[[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "#ctr to classification\n",
    "NUM_CLASSES = 3\n",
    "ctr_class_arr = []\n",
    "ctr_class_count = [0,0,0]\n",
    "for index, ctr in enumerate(ctr_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print('step:',index)\n",
    "    if ctr < 0.007:\n",
    "        ctr_class_arr.append([0,0,1])\n",
    "        ctr_class_count[2] += 1\n",
    "    elif ctr < 0.012:\n",
    "        ctr_class_arr.append([0,1,0])\n",
    "        ctr_class_count[1] += 1\n",
    "    else:\n",
    "        ctr_class_arr.append([1,0,0])\n",
    "        ctr_class_count[0] += 1\n",
    "\n",
    "print(ctr_class_count)\n",
    "print(ctr_class_arr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_sizes = [3,4,5]\n",
    "embedding_size = 300\n",
    "num_filters =100\n",
    "l2_reg_lambda = 0.0\n",
    "learning_rate = 1e-3\n",
    "\n",
    "input_x = tf.placeholder(tf.float32, [None, TITLE_LENGTH, embedding_size], name=\"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [None, NUM_CLASSES], name=\"input_y\")\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "input_x_expanded = tf.expand_dims(input_x, -1)\n",
    "l2_loss = tf.constant(0.0)\n",
    "\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "    \n",
    "        filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "        conv = tf.nn.conv2d(\n",
    "                            input_x_expanded,\n",
    "                            W,\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding=\"VALID\",\n",
    "                            name=\"conv\")\n",
    "        h = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"activate\")\n",
    "        pooled = tf.nn.max_pool(\n",
    "                            h,\n",
    "                            ksize=[1, TITLE_LENGTH - filter_size + 1, 1, 1],\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding='VALID',\n",
    "                            name=\"pool\")\n",
    "        pooled_outputs.append(pooled)\n",
    "\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "h_pool = tf.concat(pooled_outputs, 3)  #Tensor(\"concat_1:0\", shape=(?, 1, 1, 300), dtype=float32)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])  #Tensor(\"Reshape:0\", shape=(?, 300), dtype=float32)\n",
    "\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "    \n",
    "# Final (unnormalized) scores and predictions\n",
    "with tf.name_scope(\"output\"):\n",
    "    W = tf.get_variable(\n",
    "        \"W\",\n",
    "        shape=[num_filters_total, NUM_CLASSES],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]), name=\"b\")\n",
    "    l2_loss += tf.nn.l2_loss(W)\n",
    "    l2_loss += tf.nn.l2_loss(b)\n",
    "    scores = tf.nn.xw_plus_b(h_pool_flat, W, b, name=\"scores\")\n",
    "    predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "\n",
    "# # CalculateMean cross-entropy loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "    loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "    l2_loss += tf.nn.l2_loss(W)\n",
    "    l2_loss += tf.nn.l2_loss(b)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "with tf.name_scope(\"optimizing\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "# # Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training data, test data 분리\n",
    "test_data_size = 3000\n",
    "train_data_size = len(title_noun_vector_arr)-test_data_size\n",
    "\n",
    "train_input = title_noun_vector_arr[0:train_data_size]\n",
    "train_label = ctr_class_arr[0:train_data_size]\n",
    "test_input = title_noun_vector_arr[train_data_size:]\n",
    "test_label = ctr_class_arr[train_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "cost: 2.08449\n",
      "acc: 0.25\n",
      "---------\n",
      "index: 100\n",
      "cost: 1.1463\n",
      "acc: 0.3125\n",
      "---------\n",
      "index: 200\n",
      "cost: 1.15382\n",
      "acc: 0.40625\n",
      "---------\n",
      "index: 300\n",
      "cost: 1.0491\n",
      "acc: 0.421875\n",
      "---------\n",
      "============\n",
      "step: 0\n",
      "cost: 1.05124\n",
      "acc: 0.458667\n",
      "h_pool: [[ 1.          1.          0.99998093 ...,  1.          1.          1.        ]\n",
      " [ 1.          1.          1.         ...,  1.          1.          1.        ]\n",
      " [ 0.99714559  0.38904923  1.         ...,  0.99987692  0.99546796\n",
      "   0.97567594]\n",
      " ..., \n",
      " [ 1.          0.14057964  0.13284504 ...,  0.06788339  0.08506546\n",
      "   0.9998948 ]\n",
      " [ 0.9999997   1.          1.         ...,  0.99968261  0.99999994  1.        ]\n",
      " [ 0.88993782  0.14057964  0.99999976 ...,  0.99803936  0.08506546\n",
      "   0.99866962]]\n",
      "============\n",
      "index: 400\n",
      "cost: 0.927675\n",
      "acc: 0.53125\n",
      "---------\n",
      "index: 500\n",
      "cost: 1.01808\n",
      "acc: 0.5\n",
      "---------\n",
      "index: 600\n",
      "cost: 0.858155\n",
      "acc: 0.609375\n",
      "---------\n",
      "index: 700\n",
      "cost: 0.954908\n",
      "acc: 0.570312\n",
      "---------\n",
      "============\n",
      "step: 1\n",
      "cost: 1.05115\n",
      "acc: 0.458667\n",
      "h_pool: [[ 1.          1.          0.99999219 ...,  1.          1.          1.        ]\n",
      " [ 1.          1.          1.         ...,  1.          0.99991947  1.        ]\n",
      " [ 0.99655485  0.1385071   1.         ...,  0.99303597  0.99489063\n",
      "   0.95020199]\n",
      " ..., \n",
      " [ 1.          0.12298267  0.12189994 ..., -0.00224658  0.0302241\n",
      "   0.99979126]\n",
      " [ 0.99999982  1.          1.         ...,  1.          0.9999792   1.        ]\n",
      " [ 0.99670547  0.12298267  0.99999076 ...,  0.99986267  0.0302241\n",
      "   0.99703944]]\n",
      "============\n",
      "index: 800\n",
      "cost: 0.808662\n",
      "acc: 0.625\n",
      "---------\n",
      "index: 900\n",
      "cost: 0.927522\n",
      "acc: 0.546875\n",
      "---------\n",
      "index: 1000\n",
      "cost: 0.88088\n",
      "acc: 0.554688\n",
      "---------\n",
      "index: 1100\n",
      "cost: 0.986122\n",
      "acc: 0.5\n",
      "---------\n",
      "============\n",
      "step: 2\n",
      "cost: 1.05586\n",
      "acc: 0.464667\n",
      "h_pool: [[ 1.          1.          0.9998821  ...,  1.          1.          1.        ]\n",
      " [ 1.          0.99999964  0.99999976 ...,  1.          0.99963945  1.        ]\n",
      " [ 0.99953693  0.24159704  0.99999982 ...,  0.98833394  0.98941183\n",
      "   0.98163396]\n",
      " ..., \n",
      " [ 1.          0.05739029  0.0838481  ..., -0.09785877 -0.05345366\n",
      "   0.99999934]\n",
      " [ 1.          1.          1.         ...,  1.          0.9998039   1.        ]\n",
      " [ 0.94774753  0.05739029  0.99998724 ...,  0.31012207 -0.05345366\n",
      "  -0.09925724]]\n",
      "============\n",
      "index: 1200\n",
      "cost: 0.840677\n",
      "acc: 0.609375\n",
      "---------\n",
      "index: 1300\n",
      "cost: 0.781188\n",
      "acc: 0.703125\n",
      "---------\n",
      "index: 1400\n",
      "cost: 0.786322\n",
      "acc: 0.710938\n",
      "---------\n",
      "index: 1500\n",
      "cost: 0.828682\n",
      "acc: 0.570312\n",
      "---------\n",
      "============\n",
      "step: 3\n",
      "cost: 1.07344\n",
      "acc: 0.471333\n",
      "h_pool: [[ 1.          1.          0.99997097 ...,  1.          1.          1.        ]\n",
      " [ 1.          0.99999642  0.99998826 ...,  1.          0.99986458  1.        ]\n",
      " [ 0.99972785  0.82603085  0.99999994 ..., -0.1829095   0.992769\n",
      "   0.98193187]\n",
      " ..., \n",
      " [ 1.         -0.02200339  0.03388847 ..., -0.1829095  -0.14198847\n",
      "   0.99999851]\n",
      " [ 1.          1.          1.         ...,  1.          0.98948348  1.        ]\n",
      " [ 0.8139801  -0.02200339  0.99996096 ...,  0.99901199 -0.14198847\n",
      "  -0.17029342]]\n",
      "============\n",
      "index: 1600\n",
      "cost: 0.844155\n",
      "acc: 0.664062\n",
      "---------\n",
      "index: 1700\n",
      "cost: 0.717918\n",
      "acc: 0.75\n",
      "---------\n",
      "index: 1800\n",
      "cost: 0.889568\n",
      "acc: 0.625\n",
      "---------\n",
      "============\n",
      "step: 4\n",
      "cost: 1.09674\n",
      "acc: 0.469333\n",
      "h_pool: [[ 1.          1.          0.99992788 ...,  1.          1.          1.        ]\n",
      " [ 1.          0.99974829  0.99999785 ...,  1.          0.99988592  1.        ]\n",
      " [ 0.99999446  0.90452725  0.9999997  ...,  0.58436406  0.99920017\n",
      "   0.98418069]\n",
      " ..., \n",
      " [ 1.         -0.10247136 -0.01669345 ..., -0.26290408 -0.22913255  1.        ]\n",
      " [ 1.          1.          1.         ...,  1.          0.98511702  1.        ]\n",
      " [ 0.31831124 -0.10247136  0.99973166 ...,  0.98978555 -0.22913255\n",
      "   0.08456143]]\n",
      "============\n",
      "index: 1900\n",
      "cost: 0.737549\n",
      "acc: 0.65625\n",
      "---------\n",
      "index: 2000\n",
      "cost: 0.565256\n",
      "acc: 0.835938\n",
      "---------\n",
      "index: 2100\n",
      "cost: 0.598797\n",
      "acc: 0.828125\n",
      "---------\n",
      "index: 2200\n",
      "cost: 0.553467\n",
      "acc: 0.75\n",
      "---------\n",
      "============\n",
      "step: 5\n",
      "cost: 1.12969\n",
      "acc: 0.464667\n",
      "h_pool: [[ 1.          1.          0.99936056 ...,  1.          1.          1.        ]\n",
      " [ 1.          0.9997341   1.         ...,  1.          0.99997634  1.        ]\n",
      " [ 0.99998283  0.84792924  1.         ..., -0.33257994  0.99920982\n",
      "   0.95956367]\n",
      " ..., \n",
      " [ 1.         -0.17478976 -0.07543917 ..., -0.33257994 -0.30237278\n",
      "   0.99999988]\n",
      " [ 1.          1.          1.         ...,  1.          0.9905203   1.        ]\n",
      " [-0.0567877  -0.17478976  0.99866748 ...,  0.86793411 -0.30237278\n",
      "  -0.29230222]]\n",
      "============\n",
      "index: 2300\n",
      "cost: 0.585807\n",
      "acc: 0.78125\n",
      "---------\n",
      "index: 2400\n",
      "cost: 0.547241\n",
      "acc: 0.84375\n",
      "---------\n",
      "index: 2500\n",
      "cost: 0.549653\n",
      "acc: 0.84375\n",
      "---------\n",
      "index: 2600\n",
      "cost: 0.647946\n",
      "acc: 0.734375\n",
      "---------\n",
      "============\n",
      "step: 6\n",
      "cost: 1.16137\n",
      "acc: 0.460667\n",
      "h_pool: [[ 0.99999988  1.          0.99565929 ...,  1.          1.          1.        ]\n",
      " [ 1.          0.99456614  1.         ...,  1.          0.97449654  1.        ]\n",
      " [ 0.99996978  0.87778145  1.         ..., -0.37495691  0.99995655\n",
      "   0.87863815]\n",
      " ..., \n",
      " [ 1.         -0.24409531 -0.12939382 ..., -0.37495691 -0.3692061   1.        ]\n",
      " [ 1.          1.          1.         ...,  0.99999988  0.962529    1.        ]\n",
      " [ 0.45727772 -0.24409531  0.99861473 ...,  0.99399567 -0.3692061\n",
      "  -0.27612314]]\n",
      "============\n",
      "index: 2700\n",
      "cost: 0.565358\n",
      "acc: 0.765625\n",
      "---------\n",
      "index: 2800\n",
      "cost: 0.501695\n",
      "acc: 0.84375\n",
      "---------\n",
      "index: 2900\n",
      "cost: 0.402242\n",
      "acc: 0.9375\n",
      "---------\n",
      "index: 3000\n",
      "cost: 0.67975\n",
      "acc: 0.6875\n",
      "---------\n",
      "============\n",
      "step: 7\n",
      "cost: 1.20055\n",
      "acc: 0.455667\n",
      "h_pool: [[ 0.99999988  1.          0.99997777 ...,  1.          1.          1.        ]\n",
      " [ 1.          0.96846229  1.         ...,  1.          0.99986488  1.        ]\n",
      " [ 0.9999063   0.72085488  1.         ..., -0.41364965  0.99999171\n",
      "   0.84239578]\n",
      " ..., \n",
      " [ 1.         -0.3047066  -0.17753947 ..., -0.41364965 -0.41995165  1.        ]\n",
      " [ 1.          1.          1.         ...,  0.99999988 -0.41995165  1.        ]\n",
      " [ 0.29357302 -0.3047066   0.99997407 ...,  0.99996585 -0.41995165\n",
      "  -0.01337591]]\n",
      "============\n",
      "index: 3100\n",
      "cost: 0.452244\n",
      "acc: 0.851562\n",
      "---------\n",
      "index: 3200\n",
      "cost: 0.47021\n",
      "acc: 0.835938\n",
      "---------\n",
      "index: 3300\n",
      "cost: 0.339604\n",
      "acc: 0.945312\n",
      "---------\n",
      "index: 3400\n",
      "cost: 0.427901\n",
      "acc: 0.84375\n",
      "---------\n",
      "============\n",
      "step: 8\n",
      "cost: 1.25904\n",
      "acc: 0.446667\n",
      "h_pool: [[ 0.9999997   1.          0.9999972  ...,  1.          1.          1.        ]\n",
      " [ 1.          0.99530792  1.         ...,  1.          0.99576384  1.        ]\n",
      " [ 0.9999966   0.79651368  0.99999964 ..., -0.44538179  0.99999869\n",
      "   0.61705983]\n",
      " ..., \n",
      " [ 1.         -0.35434899 -0.22460639 ..., -0.44538179 -0.46121946  1.        ]\n",
      " [ 1.          1.          1.         ...,  1.         -0.23301747  1.        ]\n",
      " [ 0.86740738 -0.35434899  0.99998873 ...,  0.99922895 -0.46121946\n",
      "   0.73027092]]\n",
      "============\n",
      "index: 3500\n",
      "cost: 0.35604\n",
      "acc: 0.882812\n",
      "---------\n",
      "index: 3600\n",
      "cost: 0.308167\n",
      "acc: 0.945312\n",
      "---------\n",
      "index: 3700\n",
      "cost: 0.36834\n",
      "acc: 0.890625\n",
      "---------\n",
      "============\n",
      "step: 9\n",
      "cost: 1.31644\n",
      "acc: 0.438667\n",
      "h_pool: [[ 0.99999768  1.          0.99999523 ...,  1.          1.          1.        ]\n",
      " [ 0.99999964  0.95466411  1.         ...,  1.          0.98625219  1.        ]\n",
      " [ 0.99999785  0.84997487  1.         ..., -0.47008255  1.          0.77387446]\n",
      " ..., \n",
      " [ 1.         -0.39243636 -0.26570648 ..., -0.47008255 -0.49294052  1.        ]\n",
      " [ 1.          1.          1.         ...,  1.         -0.49294052  1.        ]\n",
      " [ 0.90492529 -0.39243636  0.99999166 ...,  0.99969059 -0.49294052\n",
      "   0.49060225]]\n",
      "============\n",
      "index: 3800\n",
      "cost: 0.338438\n",
      "acc: 0.859375\n",
      "---------\n",
      "index: 3900\n",
      "cost: 0.30458\n",
      "acc: 0.929688\n",
      "---------\n",
      "index: 4000\n",
      "cost: 0.221547\n",
      "acc: 0.976562\n",
      "---------\n",
      "index: 4100\n",
      "cost: 0.365125\n",
      "acc: 0.867188\n",
      "---------\n",
      "============\n",
      "step: 10\n",
      "cost: 1.37856\n",
      "acc: 0.435\n",
      "h_pool: [[ 0.99996889  1.          0.99999452 ...,  1.          1.00000012  1.        ]\n",
      " [ 0.99999964  0.94999683  1.         ...,  1.          0.99134701  1.        ]\n",
      " [ 0.99999732  0.80055404  1.         ..., -0.48836613  1.          0.83586895]\n",
      " ..., \n",
      " [ 1.         -0.42526203 -0.29590186 ..., -0.48836613 -0.51846319  1.        ]\n",
      " [ 1.          1.          1.         ...,  1.         -0.51846319  1.        ]\n",
      " [ 0.59799159 -0.42526203  0.99993205 ...,  0.99995202 -0.51846319\n",
      "   0.92017531]]\n",
      "============\n",
      "index: 4200\n",
      "cost: 0.343069\n",
      "acc: 0.882812\n",
      "---------\n",
      "index: 4300\n",
      "cost: 0.229029\n",
      "acc: 0.953125\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "#train & test\n",
    "\n",
    "batch_size = 128\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "index = 0\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter('tensorboard/train',sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('tensorboard/test')\n",
    "    sess.run(init)\n",
    "    for step in range(100):\n",
    "        for start, end in zip(range(0, len(train_input), batch_size), range(batch_size, len(train_input), batch_size)):\n",
    "            cost, acc, tb_summary, h_pool, _ = sess.run([loss, accuracy, merged, h_pool_flat, optimizer], feed_dict={input_x: train_input[start:end], input_y: train_label[start:end], dropout_keep_prob: 0.5})\n",
    "            if index % 100 == 0:\n",
    "                train_writer.add_summary(tb_summary, index)\n",
    "                print('index:',index)\n",
    "                print('cost:',cost)\n",
    "                print('acc:',acc)\n",
    "                print('---------')\n",
    "            index += 1\n",
    "        t_cost, t_acc, t_tb_summary, t_h_pool = sess.run([loss, accuracy, merged, h_pool_flat], feed_dict={input_x: test_input, input_y: test_label, dropout_keep_prob: 1})\n",
    "        test_writer.add_summary(t_tb_summary, step)\n",
    "        print('============')\n",
    "        print('step:',step)\n",
    "        print('cost:',t_cost)\n",
    "        print('acc:',t_acc)\n",
    "        print('h_pool:',t_h_pool)\n",
    "        print('============')\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
