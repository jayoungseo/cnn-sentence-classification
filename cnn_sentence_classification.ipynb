{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['초강력 워터프루프', '셔츠 초간단 연출법', '센스 넘치는 여행 소품들', '부위별 셀프 운동법', '이국주 퍼스널리티 오전', '강동원 인형놀이 오후', '호위무사 박수경 팬클럽까지 예쁘면 모든게 용서되나 경악 저녁', '빈 라덴 사살됐다 살아있다 여전히 풀리지 않는 미스터리 저녁', '남친에게 이런 짓까지 미녀스타의 충격적 실체 저녁', '택배등기 반송메시지 조심해야 하는 이유 저녁']\n",
      "[0.0211, 0.0144, 0.0091, 0.0209, 0.0177, 0.0091, 0.0338, 0.0142, 0.0901, 0.0148]\n"
     ]
    }
   ],
   "source": [
    "#파일에서 데이터 로드\n",
    "title_arr = []\n",
    "ctr_arr = []\n",
    "with open('../data/title_ctr.csv','r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break;\n",
    "        title_info = line.split(',')\n",
    "        title = title_info[2]\n",
    "        if len(title) > 0:\n",
    "            title_arr.append(title)\n",
    "            ctr_arr.append(float(title_info[3].replace('\\n','')))\n",
    "        \n",
    "print(title_arr[:10])\n",
    "print(ctr_arr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "step: 10000\n",
      "step: 20000\n",
      "step: 30000\n",
      "step: 40000\n",
      "step: 50000\n",
      "[['초', '강력', '워터', '프루프'], ['셔츠', '초', '간단', '연출', '법'], ['센스', '여행', '소품'], ['부위', '별', '셀프', '운동', '법'], ['이국주', '퍼스', '널리', '티', '오전'], ['강동원', '인형', '오후'], ['호위', '무사', '박수경', '팬클럽', '모든', '용서', '경악', '저녁'], ['빈', '라덴', '사살', '미스터리', '저녁'], ['남친', '짓', '미녀', '스타', '충격', '실체', '저녁'], ['택배', '등기', '반송', '메시지', '이유', '저녁']]\n"
     ]
    }
   ],
   "source": [
    "#명사 추출\n",
    "from konlpy.tag import Twitter\n",
    "twitter_nlp = Twitter()\n",
    "\n",
    "title_noun_arr = []\n",
    "for index, title in enumerate(title_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print('step:',index)\n",
    "    title_noun_arr.append(twitter_nlp.nouns(title)) #명사 추출\n",
    "print(title_noun_arr[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "step: 10000\n",
      "step: 20000\n",
      "step: 30000\n",
      "step: 40000\n",
      "step: 50000\n"
     ]
    }
   ],
   "source": [
    "#벡터화 using word2vec\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "MIN_NOUN_VECTOR_VALUE = -10.0\n",
    "MAX_NOUN_VECTOR_VALUE = 10.0\n",
    "NOUN_VECTOR_SIZE = 300\n",
    "\n",
    "def generate_random_noun_vector():\n",
    "    return np.random.uniform(low=MIN_NOUN_VECTOR_VALUE, high=MAX_NOUN_VECTOR_VALUE, size=(NOUN_VECTOR_SIZE,))\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec.load('../data/w2v_model_wiki_kor')\n",
    "w2v_model['남자']\n",
    "\n",
    "title_noun_vector_arr = []\n",
    "for index, title_nouns in enumerate(title_noun_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print('step:',index)\n",
    "    noun_vector_arr = []\n",
    "    for noun in title_nouns:\n",
    "        try:\n",
    "            noun_vector = w2v_model[noun]\n",
    "        except Exception as e:\n",
    "            noun_vector = generate_random_noun_vector()\n",
    "        noun_vector_arr.append(noun_vector)\n",
    "    title_noun_vector_arr.append(noun_vector_arr)\n",
    "\n",
    "# print(title_noun_vector_arr[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "step: 10000\n",
      "step: 20000\n",
      "step: 30000\n",
      "step: 40000\n",
      "step: 50000\n"
     ]
    }
   ],
   "source": [
    "#pedding\n",
    "\n",
    "TITLE_LENGTH = 25\n",
    "\n",
    "def generate_zero_noun_vector():\n",
    "    return np.random.uniform(low=0.0, high=0.0, size=(NOUN_VECTOR_SIZE,))\n",
    "\n",
    "for index, title_noun_vector in enumerate(title_noun_vector_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print('step:',index)\n",
    "    while len(title_noun_vector) < 25:\n",
    "        title_noun_vector.append(generate_zero_noun_vector())\n",
    "    title_noun_vector = title_noun_vector[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "step: 10000\n",
      "step: 20000\n",
      "step: 30000\n",
      "step: 40000\n",
      "step: 50000\n",
      "[17005, 16347, 18116]\n",
      "[[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "#ctr to classification\n",
    "NUM_CLASSES = 3\n",
    "ctr_class_arr = []\n",
    "ctr_class_count = [0,0,0]\n",
    "for index, ctr in enumerate(ctr_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print('step:',index)\n",
    "    if ctr < 0.007:\n",
    "        ctr_class_arr.append([0,0,1])\n",
    "        ctr_class_count[2] += 1\n",
    "    elif ctr < 0.012:\n",
    "        ctr_class_arr.append([0,1,0])\n",
    "        ctr_class_count[1] += 1\n",
    "    else:\n",
    "        ctr_class_arr.append([1,0,0])\n",
    "        ctr_class_count[0] += 1\n",
    "\n",
    "print(ctr_class_count)\n",
    "print(ctr_class_arr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_sizes = [3,4,5]\n",
    "embedding_size = 300\n",
    "num_filters =100\n",
    "l2_reg_lambda = 0.0\n",
    "learning_rate = 1e-3\n",
    "\n",
    "input_x = tf.placeholder(tf.float32, [None, TITLE_LENGTH, embedding_size], name=\"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [None, NUM_CLASSES], name=\"input_y\")\n",
    "input_x_expanded = tf.expand_dims(input_x, -1)\n",
    "l2_loss = tf.constant(0.0)\n",
    "\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "    \n",
    "        filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "        conv = tf.nn.conv2d(\n",
    "                            input_x_expanded,\n",
    "                            W,\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding=\"VALID\",\n",
    "                            name=\"conv\")\n",
    "        h = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"activate\")\n",
    "        pooled = tf.nn.max_pool(\n",
    "                            h,\n",
    "                            ksize=[1, TITLE_LENGTH - filter_size + 1, 1, 1],\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding='VALID',\n",
    "                            name=\"pool\")\n",
    "        pooled_outputs.append(pooled)\n",
    "\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "h_pool = tf.concat(pooled_outputs, 3)  #Tensor(\"concat_1:0\", shape=(?, 1, 1, 300), dtype=float32)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])  #Tensor(\"Reshape:0\", shape=(?, 300), dtype=float32)\n",
    "\n",
    "# Final (unnormalized) scores and predictions\n",
    "with tf.name_scope(\"output\"):\n",
    "    W = tf.get_variable(\n",
    "        \"W\",\n",
    "        shape=[num_filters_total, NUM_CLASSES],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]), name=\"b\")\n",
    "    l2_loss += tf.nn.l2_loss(W)\n",
    "    l2_loss += tf.nn.l2_loss(b)\n",
    "    scores = tf.nn.xw_plus_b(h_pool_flat, W, b, name=\"scores\")\n",
    "    predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "\n",
    "# # CalculateMean cross-entropy loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "    loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "    l2_loss += tf.nn.l2_loss(W)\n",
    "    l2_loss += tf.nn.l2_loss(b)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "with tf.name_scope(\"optimizing\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "# # Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "cost: 1.29363\n",
      "acc: 0.28125\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "index = 0\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter('tensorboard/train',sess.graph)\n",
    "    sess.run(init)\n",
    "    for step in range(100):\n",
    "        for start, end in zip(range(0, len(title_noun_vector_arr), batch_size), range(batch_size, len(title_noun_vector_arr), batch_size)):\n",
    "            cost, acc, tb_summary, h_pool, _ = sess.run([loss, accuracy, merged, h_pool_flat, optimizer], feed_dict={input_x: title_noun_vector_arr[start:end], input_y: ctr_class_arr[start:end]})\n",
    "            if index % 100 == 0:\n",
    "                train_writer.add_summary(tb_summary, index)\n",
    "                print('index:',index)\n",
    "                print('cost:',cost)\n",
    "                print('acc:',acc)\n",
    "                print('---------')\n",
    "            index += 1\n",
    "        print('============')\n",
    "        print('step:',step)\n",
    "        print('cost:',cost)\n",
    "        print('acc:',acc)\n",
    "        print('h_pool:',h_pool)\n",
    "        print('============')\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
